# Проект: Подготовка данных резюме с hh.ru для модели

## Оглавление
[1. Описание проекта](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Описание-проекта)  
[2. Краткая информация о данных](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Краткая-информация-о-данных)  
[3. Этапы работы над проектом](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Этапы-работы-над-проектом)  
[4. Результат](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Результат)  
[5. Использованные инструменты и библиотеки](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Использованные-инструменты-и-библиотеки) 
[6. Дополнительные источники](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Дополнительные-источники) 


### Описание проекта
Мы иммем исходный датасет данных с HeadHunter о соискателях, однако часть соискателей не указывает желаемую заработную плату, когда составляет своё резюме. Поэтому необходимо преобразовать, исследовать и очистить данные, чтобы в будущем можно было построить модель, которая бы автоматически определяла примерный уровень заработной платы, подходящей пользователю, исходя из информации, которую он указал о себе.

Основные этапы исследования и обработки данных следующие:
1. Базовый анализ структуры данных;
2. Преобразование данных
3. Разведывательный анализ;
4. Очистка данных.

### Краткая информация о данных
База резюме, выгруженная с сайта поиска вакансий hh.ru в формате csv-файла. Ссылка на исходный датасет находится в блоке 6. Дополнительные источники. Размер исходного датасета составляет (44744, 12).

### Этапы работы над проектом
**1. Базовый анализ структуры данных**  
**Цель:** понять, как устроены признаки в данных и какие типы они имеют, чтобы произвести дальнейшие преобразования.

**2. Преобразование данных**  
**Цель:** обработать «сырые» данные, так как часть признаков представлены в неудобном для анализа и очистки формате

**3. Разведывательный анализ**  
**Цель:** провести разведывательный анализ, предназначенный для выявления связей между признаками, выявления закономерностей, определения распределений признаков, поиска аномалий и других дефектов данных.

**4. Очистка данных**  
**Цель:** очистить датасет от дубликатов, выбросов и пропусков.


### Результат
В результате проекта исходный датасет был проанализирован на качество данных, преобразованы и добавлены новые графы с нужной информацией. Проведен разведывательный анаиз с помощью библиотек визуализации, в реузбльтате чего выявлены взаимосвязи уровня желаемой ЗП от ряда признаков.  
Как итог проекта мы получали очищенный от дубликатов, выбросов и пропусков датасет с резюме соискателей, готовый для подгрузки в модель.

### Использованные инструменты и библиотеки
* numpy (1.24.0)
* pandas (1.5.2)
* matplotlib (3.6.3)
* seaborn (0.12.2)
* plotly (5.13.0)

### Дополнительные источники
* [Ссылка на датасет](https://console.firebase.google.com/project/data-hh/storage/data-hh.appspot.com/files)

:arrow_up:[к Оглавлению](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Оглавление)