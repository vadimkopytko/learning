# Проект: Подготовка данных резюме с hh.ru для модели

## Оглавление
[1. Описание проекта;](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Описание-проекта)  
[2. Краткая информация о данных;](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Краткая-информация-о-данных)  
[3. Этапы работы над проектом;](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Этапы-работы-над-проектом)  
[4. Результаты;](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Результаты)  
[5. Использованные инструменты и библиотеки;](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Использованные-инструменты-и-библиотеки)   
[6. Дополнительные источники.](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Дополнительные-источники) 


### Описание проекта
Мы имеем исходный датасет данных с HeadHunter о соискателях, однако часть соискателей не указывает желаемую заработную плату, когда составляет своё резюме. Поэтому необходимо преобразовать, исследовать и очистить данные, чтобы в будущем можно было построить модель, которая бы автоматически определяла примерный уровень заработной платы, подходящей пользователю, исходя из информации, которую он указал о себе.

Основные этапы исследования и обработки данных следующие:
1. Базовый анализ структуры данных;
2. Преобразование данных
3. Разведывательный анализ;
4. Очистка данных.

### Краткая информация о данных
База резюме, выгруженная с сайта поиска вакансий hh.ru в формате csv-файла. Ссылка на исходный датасет находится в блоке 6. Дополнительные источники.  
Размер исходного датасета составляет (44744, 12).

### Этапы работы над проектом
**1. Базовый анализ структуры данных**  
**Цель:** понять, как устроены признаки в данных, и какие типы они имеют, чтобы произвести дальнейшие преобразования.

**2. Преобразование данных**  
**Цель:** обработать «сырые» данные, так как часть признаков представлены в неудобном для анализа и очистки формате.

**3. Разведывательный анализ**  
**Цель:** выявление связей между признаками, закономерностей, определение распределений признаков, поиск аномалий и других дефектов данных.

**4. Очистка данных**  
**Цель:** очистить датасет от дубликатов, выбросов и пропусков.  

### Результаты
**1. Базовый анализ структуры данных**  
**Результаты этапа:** проанализирован датасет, а так же поля и их наполнение. Собрана основная статистическая информация по каждому полю. 

**2. Преобразование данных**  
**Результаты этапа:** проделано преобразование данных: 
* разбиты поля, которые содержали несколько признаков (например, пол и возраст) при помощи лямбда-функций;
* произведено преобразование поля Опыт работы из текстового формата в количество месяцев  
``` python
def get_experience(arg):
    if pd.isna(arg):
        res = arg
    elif arg == 'Не указано':
        res = float('NaN')
    else:
        year = ''
        month = ''
        list_arg = arg.split(' ')[2:6]
        for i in range(len(list_arg)):
            if list_arg[i].startswith('л') or list_arg[i].startswith('г'):
                year = list_arg[i-1]
            elif list_arg[i].startswith('м'):
                month = list_arg[i-1]
        if year == '': year = 0
        if month == '': month = 0
        res = int(year) * 12 + int(month)
    return res
```
* поле "Город, станцию метро, готовность к командировкам и переезду" разбито на отдельные поля "Город", "Готовность к командировкам" и "Готовность к переезду" при помощи лямбда-функций;
* преобразованы некоторые категориальные признаки методом One Hot Encoding при помощи лямбда-функции;
* преобразовано поле заработной платы в разных валютах к сумме заработной платы в рублях.

**3. Разведывательный анализ** 
**Результаты:** были построены графики с помощью библиотек визуализации: seaborn и plotly. На основании графиков были выявлены влияния категориальных признаков на уровень желаемой заработной платы. Так же графики помогли выявить явные выбросы (например, возраст соискателей = 100 лет, опыт работы в годах превышает возраст соискателей и пр.).  

**4. Очистка данных**  
**Результаты:** были удалены полные дубликаты, удалены строки с пропусками, а так же по полю "Опыт работы (месяц)" пропуски заменены на медианное значение. Итоговый этап - очистка датасета от выбросов, которые были выявлены на этапе разведывательного анализа.

**Итоговый результат:**
В результате проекта исходный датасет был проанализирован на качество данных, преобразованы и добавлены новые графы с нужной информацией. Проведен разведывательный анализ с помощью библиотек визуализации, в результате чего выявлены взаимосвязи уровня желаемой ЗП от ряда признаков.  
Как итог проекта мы получали очищенный от дубликатов, выбросов и пропусков датасет с резюме соискателей, готовый для подгрузки в модель.

### Использованные инструменты и библиотеки
* numpy (1.24.0)
* pandas (1.5.2)
* matplotlib (3.6.3)
* seaborn (0.12.2)
* plotly (5.13.0)  

### Дополнительные источники
* [Ссылка на датасет на Firebase](https://console.firebase.google.com/project/data-hh/storage/data-hh.appspot.com/dst-3.0_16_1_hh_database.csv)  
* [Ссылка на графики в plotly в формате png и html](https://github.com/vadimkopytko/learning/tree/main/SkillFactory/PY-17_PROJECT-1/Charts)

:arrow_up:[к Оглавлению](https://github.com/vadimkopytko/learning/blob/main/SkillFactory/PY-17_PROJECT-1/README.md#Оглавление)